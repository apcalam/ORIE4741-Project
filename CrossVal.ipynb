{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"ProgressBars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, LinearAlgebra, Plots\n",
    "# bring packages into main namespace\n",
    "using DataFrames             # Data tables are called \"DataFrames\"\n",
    "using StatsPlots             # load plotting packages \n",
    "using Statistics             # basic statistical functions\n",
    "using CSV                    # tools for working with CSV files\n",
    "using Plots, Random, LinearAlgebra, Statistics, SparseArrays\n",
    "using ProgressBars\n",
    "include(\"proxgrad.jl\")\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSV.read(\"Consumer_Airfare_Report__Table_1a_-_All_U.S._Airport_Pair_Markets.csv\")\n",
    "dataset = dropmissing(dataset)\n",
    "dataset[:when] = dataset[:Year] .+ ((dataset[:quarter] .-1) ./ 4)\n",
    "n = size(dataset,1)\n",
    "r = .1\n",
    "cuttoff = Int(round(n*r))\n",
    "idxs = rand(1:n,n)\n",
    "\n",
    "# We need to hold out a test dataset to report final results on, that is not used at all for model selection.\n",
    "# Within k-fold cross validation, train and val datasets will be formed from this main train dataset. \n",
    "test_dataset  = dataset[idxs[1:cuttoff],:]\n",
    "train_dataset = dataset[idxs[cuttoff+1:n],:]\n",
    "\n",
    "n_train = size(train_dataset,1)\n",
    "n_test = size(test_dataset,1)\n",
    "\n",
    "@assert  n_train + n_test == n\n",
    "@show n_train, n_test\n",
    "head(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do k-fold cross validation and return the average error_metric on the validation set accross the k folds.\n",
    "function cross_val(featurizer, loss, regularizer, stepsize, error_metric; k=10, dataset=train_dataset)\n",
    "    X,y = featurizer(dataset)\n",
    "    n = size(dataset,1)\n",
    "    r = Int(round(n / k))\n",
    "    idxs = rand(1:n,n) # to shuffle the dataset\n",
    "    error = 0\n",
    "    for i in tqdm(1:k)\n",
    "        val_idxs = r*(i-1)+1:min(r*i,n)\n",
    "        tr_low = 1:r*(i-1)\n",
    "        tr_high = r*i+1:n\n",
    "        if (i == 1)\n",
    "            tr_idxs = tr_high\n",
    "        elseif (i == k)\n",
    "            tr_idxs = tr_low\n",
    "        else\n",
    "            tr_idxs = [tr_low ; tr_high ]\n",
    "        end\n",
    "        # @show i, val_idxs\n",
    "        # @show tr_low, tr_high\n",
    "        X_tr = X[idxs[tr_idxs],:]\n",
    "        y_tr = y[idxs[tr_idxs]]\n",
    "        \n",
    "        X_val = X[idxs[val_idxs],:]\n",
    "        y_val = y[idxs[val_idxs]]\n",
    "        \n",
    "        w = proxgrad(loss, regularizer, X_tr, y_tr; stepsize=stepsize) \n",
    "        ŷ_val = X_val * w\n",
    "        # @show size(y_val)\n",
    "        # @show size(ŷ_val)\n",
    "        error += error_metric(ŷ_val, y_val)\n",
    "    end\n",
    "    return error / k\n",
    "end\n",
    "\n",
    "# For each model in models, do k-fold cross validation and calculate the average error_metric\n",
    "# on the val set accross the k-folds.\n",
    "# Each model in model is a tuple of the form (featurizer, loss, regularizer, stepsize),\n",
    "# where a featurizer is a funciton that takes in a dataset and returns X,y. \n",
    "# Returns errors for each model, and the index of the best model\n",
    "function test_models(models, error_metric;k=10, dataset=train_dataset)\n",
    "    errors = []\n",
    "    for model in models\n",
    "        error = cross_val(model...,error_metric;k=k,dataset=dataset)\n",
    "        errors = [errors; error]\n",
    "    end\n",
    "    i = argmin(errors)\n",
    "    println(\"The best model is model \",i)\n",
    "    return errors,i\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = [:carrier_lg, :carrier_low, :airportid_1, :airportid_2]\n",
    "cat_labels_sub = [:carrier_lg, :carrier_low]\n",
    "\n",
    "#Sets of all categories in a particular column\n",
    "cats_sets = [unique(dataset[:, label]) for label in cat_labels]\n",
    "cats_sets_sub = [unique(dataset[:, label]) for label in cat_labels_sub]\n",
    "\n",
    "\"Computes a onehot vector for every entry in column given a set of categories cats\"\n",
    "function onehot(column, cats=unique(column))\n",
    "    result = zeros(length(column), length(cats))\n",
    "    for i = 1:length(column)\n",
    "        for j =1:length(cats)\n",
    "            if column[i] === cats[j]\n",
    "                result[i, j] = 1\n",
    "            end    \n",
    "        end\n",
    "    end\n",
    "    result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_real = [\n",
    "    :when, \n",
    "    :citymarketid_1, \n",
    "    :citymarketid_2,\n",
    "    :airportid_1,\n",
    "    :airportid_2,\n",
    "    :nsmiles,\n",
    "    :passengers,\n",
    "    :large_ms,\n",
    "    :fare_lg,\n",
    "    :lf_ms,\n",
    "    :fare_low\n",
    "]\n",
    "labels_real_sub = [\n",
    "    :when, \n",
    "    :nsmiles,\n",
    "    :passengers,\n",
    "    :large_ms,\n",
    "    :fare_lg,\n",
    "    :lf_ms,\n",
    "    :fare_low\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline featurizer\n",
    "function feats_0(dataset)\n",
    "    X = [dataset[:when] dataset[:airportid_1] dataset[:airportid_2] ones(size(dataset,1))]\n",
    "    y = dataset[:fare]\n",
    "    return X,y\n",
    "end\n",
    "\n",
    "# Only numeric features (treating airportid and cityid as numeric)\n",
    "function feats_1(dataset)\n",
    "    real_vals = convert(Matrix, dataset[labels_real])\n",
    "    X = hcat(real_vals, ones(size(dataset,1)))\n",
    "    y = dataset[:fare]\n",
    "    return X,y\n",
    "end\n",
    "\n",
    "# Numeric and categorical combined (treating airportid and cityid as numerical)\n",
    "function feats_2(dataset)\n",
    "    real_vals = convert(Matrix, dataset[labels_real])\n",
    "    cat_vals = hcat([onehot(dataset[:, cat_labels_sub[i]], cats_sets_sub[i]) for i in 1:size(cat_labels_sub, 1)]...)\n",
    "    X = hcat(cat_vals, real_vals, ones(size(dataset,1)))\n",
    "    y = dataset[:fare]\n",
    "    return X,y\n",
    "end\n",
    "\n",
    "# Numeric and categorical combined (treating airportid and cityid as categorical)\n",
    "function feats_3(dataset)\n",
    "    real_vals = convert(Matrix, dataset[labels_real_sub])\n",
    "    cat_vals = hcat([onehot(dataset[:, cat_labels[i]], cats_sets[i]) for i in 1:size(cat_labels, 1)]...)\n",
    "    X = hcat(cat_vals, real_vals, ones(size(dataset,1)))\n",
    "    y = dataset[:fare]\n",
    "    return X,y\n",
    "end\n",
    "\n",
    "MSE(L1,L2) = sum((L1.-L2).^2) / size(L1,1)\n",
    "\n",
    "models = [\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .1),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .1),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .1),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .2),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .2),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .2),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .3),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .3),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .3),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .4),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .4),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .4),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .5),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .5),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .5),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .6),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .6),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .6),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .7),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .7),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .7),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .8),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .8),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .8),\n",
    "    \n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.25*QuadReg(), .9),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.5*QuadReg(), .9),\n",
    "    (feats_3, 1/n_train*QuadLoss(), 0.75*QuadReg(), .9),\n",
    "]\n",
    "\n",
    "errors,i = test_models(models,MSE;k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call external extrapolation on models"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
